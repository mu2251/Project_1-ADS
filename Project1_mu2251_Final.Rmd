---
title: "Presidential Inaugural Speech Analyses"
output: html_notebook
---

Plan:
(1) Analyzing the whole presidential inaugural speech as a whole (providing the trends
over time).
(2) Analyzing American politics after 1980 through presidential inaugural speeches. 
The analyses for the "storyline" are based on each parties, especially after the 
1980. The speech analyses after 1980 are consequential because the historians claim that
after 1980, there is a great shift in American politics. The Republican Party generally 
advocates for small-government, with the exception of security and borders. The 
Democratic Party represents the big-government that generally concerns about welfare.
In this qualitative analyses, we would like to show whether these generalization 
is true or not. 
(3) Analyzing speech by Founding Fathers. 

Reference:
Micklethwait, J., & Wooldridge, A. (2015). The fourth revolution: the global race to reinvent the state. New York, NY: Penguin Books. 

Code Reference:
Wk2-Tutorial-TextMining. (n.d.). Retrieved January 30, 2018, from https://github.com/TZstatsADS/ADS_Teaching/blob/master/Tutorials/wk2-TextMining/doc/wk2-Tutorial-TextMining.Rmd



0.) Package Preparation
Download all the packages needed for data analyses purposes. 
```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}


# load packages
library("rvest")
library("tibble")
# You may need to run
# sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
# in order to load qdap
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("xml2")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

1.) Find the inaugural speech date and make sure that every speeches are downloaded
```{r}
### Inaugural speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches. 
inaug=f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
```

2.) Using speech metadata
```{r}
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
```

3.) Scrap the text
```{r}
speech.list=inaug.list
speech.list$type=c(rep("inaug", nrow(inaug.list)))
speech.url=inaug
speech.list=cbind(speech.list, speech.url)
```

```{r}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
  text <- read_html(speech.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  speech.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/fulltext/", 
                     speech.list$type[i],
                     speech.list$File[i], "-", 
                     speech.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}
```

4.) Generate list of sentences
```{r, message=FALSE, warning=FALSE}
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    # colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```

Some non-sentences exist in raw data due to erroneous extra end-of-sentence marks. 
```{r}
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count)) 

```

5.) Data Analysis 
Average length of speech Data Visualization each president
```{r}
sentence.list$FileOrdered=reorder(sentence.list$File,sentence.list$word.count, mean,order=T)

beeswarm(word.count~FileOrdered, 
         data=sentence.list,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.8, cex.lab=0.8,
         spacing=5/nlevels(sentence.list$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Presidential speeches")

```
Plot the word length of a sentence according to time
#Maybe earlier presidents used longer sentences

6.) Sentiment Analysis
```{r}
presidentNames <- unique(sentence.list$File)
for(i in 1:length(presidentNames)){
        print(i)
        if(presidentNames[i] == 'GroverCleveland-II'){
        f.plotsent.len(In.list=sentence.list, InFile=presidentNames[i], 
                       InType="inaug", InTerm=2, President=presidentNames[i])  
        }
        else{
        f.plotsent.len(In.list=sentence.list, InFile=presidentNames[i], 
                       InType="inaug", InTerm=1, President=presidentNames[i])  
        }
        }
```

Emotionally charged sentences per president:
```{r}
for(i in 1: length(presidentNames)){
print(presidentNames[i])
speech.df=tbl_df(sentence.list)%>%
  filter(File==presidentNames[i], type=="inaug", word.count>=4)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
print(as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)]))
print("")
}
```

Clustering of emotions: (from Tutorials)
```{r, fig.width=2, fig.height=2}
heatmap.2(cor(sentence.list%>%filter(type=="inaug")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none")

par(mar=c(4, 6, 2, 1))
emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
```
```{r, fig.height=3.3, fig.width=3.7}
presid.summary=tbl_df(sentence.list)%>%
  filter(type=="inaug")%>%
  #group_by(paste0(type, File))%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
    #negative=mean(negative),
    #positive=mean(positive)
  )

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
              5)
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = presid.summary[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

Sort Presidents based on emotions
```{r}
presid.summary[order(presid.summary$fear),]
presid.summary[order(presid.summary$joy),]
```

7.) Topic Modelling

Divide topic based on political parties 

```{r}
rep.sentence.list = sentence.list[sentence.list$Party == 'Republican',]
dem.sentence.list = sentence.list[sentence.list$Party == 'Democratic',]

## Founding Fathers
founding.fathers <- c('GeorgeWashington', 'JohnAdams', 'ThomasJefferson', 'JamesMadison')
fathers.i <- sentence.list$FileOrdered %in% founding.fathers
fathers.list <- sentence.list[fathers.i,]
fathers.list$Party <- 'FoundingFathers'

political.list <- list(rep.sentence.list, dem.sentence.list, fathers.list)
```

Party LDA
```{r}
for (i in 1:length(political.list)){
        this.list <- political.list[[i]]
        this.party = unique(this.list$Party)
        this.party <- this.party[!is.na(this.party)]
        # Party LDA
        corpus.list=this.list[2:(nrow(this.list)-1), ]
        sentence.pre=this.list$sentences[1:(nrow(this.list)-2)]
        sentence.post=this.list$sentences[3:(nrow(this.list)-1)]
        corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
        rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
        rm.rows=c(rm.rows, rm.rows-1)
        #corpus.list=corpus.list[-rm.rows, ]
        
        # Text Mining
        docs <- Corpus(VectorSource(corpus.list$snipets))
        
        
        
        #remove potentially problematic symbols
        docs <-tm_map(docs,content_transformer(tolower))
       
        
        #remove punctuation
        docs <- tm_map(docs, removePunctuation)
        
        
        #Strip digits
        docs <- tm_map(docs, removeNumbers)
        
        
        #remove stopwords
        docs <- tm_map(docs, removeWords, stopwords("english"))
        
        
        #remove whitespace
        docs <- tm_map(docs, stripWhitespace)
        
        
        #Stem document
        docs <- tm_map(docs,stemDocument)
        
        
        ### Topic modeling
        dtm <- DocumentTermMatrix(docs)
        #convert rownames to filenames#convert rownames to filenames
        rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                               corpus.list$Term, corpus.list$sent.id, sep="_")
        
        rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
        
        dtm  <- dtm[rowTotals> 0, ]
        corpus.list=corpus.list[rowTotals>0, ]
        
        
        # Run LDA
        #Set parameters for Gibbs sampling
        burnin <- 0
        iter <- 100
        thin <- 50
        seed <- 013018
        nstart <- 1
        best <- TRUE
        
        #Number of topics
        k <- 8
        
        #Run LDA using Gibbs sampling
        ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                          seed = seed, best=best,
                                                          burnin = burnin, iter = iter, 
                                                          thin=thin))
        #write out results
        #docs to topics
        ldaOut.topics <- as.matrix(topics(ldaOut))
        table(c(1:k, ldaOut.topics))
        write.csv(ldaOut.topics,file=paste0("LDAGibbs",k,this.party,"DocsToTopics.csv"))
      
        #top 6 terms in each topic
        ldaOut.terms <- as.matrix(terms(ldaOut,20))
        write.csv(ldaOut.terms,file=paste0("LDAGibbs",k,this.party,"TopicsToTerms.csv"))
        print(paste("Words Distributions for ", this.party))
        print(ldaOut.terms)
        #probabilities associated with each topic assignment
        topicProbabilities <- as.data.frame(ldaOut@gamma)
        write.csv(topicProbabilities,file=paste0("LDAGibbs",k,this.party,"TopicProbabilities.csv"))
}

```



```{r}
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
```

Text mining
```{r}
docs <- Corpus(VectorSource(corpus.list$snipets))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```

Basic text processing
```{r}
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Strip digits
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove whitespace
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Stem document
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```





Topic modelling
```{r}
dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                       corpus.list$Term, corpus.list$sent.id, sep="_")

rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]

```

Run LDA

```{r}
#Set parameters for Gibbs sampling
burnin <- 0
iter <- 100
thin <- 50
seed <- 013018
nstart <- 1
best <- TRUE

#Number of topics
k <- 9

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
```

```{r}
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../out/LDAGibbs",k,"DocsToTopics.csv"))

#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file=paste("../out/LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../out/LDAGibbs",k,"TopicProbabilities.csv"))

#LDA Terms
ldaOut.terms
```

```{r}
#docs to topics
        ldaOut.topics <- as.matrix(topics(ldaOut))
        table(c(1:k, ldaOut.topics))
        write.csv(ldaOut.topics,file=paste0("LDAGibbs",k,this.party,"DocsToTopics.csv"))
        
        #top 6 terms in each topic
        ldaOut.terms <- as.matrix(terms(ldaOut,20))
        write.csv(ldaOut.terms,file=paste0("LDAGibbs",k,this.party,"TopicsToTerms.csv"))
        
        #probabilities associated with each topic assignment
        topicProbabilities <- as.data.frame(ldaOut@gamma)
        write.csv(topicProbabilities,file=paste0("LDAGibbs",k,this.party,"TopicProbabilities.csv"))
```

```{r}
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
```










```{r}
library(tidytext)
library(wordcloud)
preprocess_data<- function(sentence.list){
  
  docs <- Corpus(VectorSource(corpus.list$snipets))
  #remove potentially problematic symbols
  docs <-tm_map(docs,content_transformer(tolower))
  
  #remove punctuation
  docs <- tm_map(docs, removePunctuation)
  
  #Strip digits
  docs <- tm_map(docs, removeNumbers)
  
  #remove stopwords
  docs <- tm_map(docs, removeWords, stopwords("english"))
  
  #remove whitespace
  docs <- tm_map(docs, stripWhitespace)
  
  #Stem document
  docs <- tm_map(docs,stemDocument)
}

docs <- preprocess_data(dem.sentence.list)
tdm.all<-TermDocumentMatrix(docs)
tdm.tidy=tidy(tdm.all)
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))

wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(4,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues")
         )

```








