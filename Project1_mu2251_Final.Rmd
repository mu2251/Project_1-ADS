---
title: "Presidential Inaugural Speech Analysis, by Michael Utomo"
output: html_notebook
---

Content:
(1) Analysis of the number of words in a sentence presidential speeches.
(2) Analysis of  American politics through presidential inaugural speeches. 
The analysis for the "storyline" are based on each parties. The Republican Party generally 
advocates for small-government, with the exception of security and borders. The 
Democratic Party represents the big-government that generally concerns about welfare.
In this qualitative analyses, we would like to show whether these generalization 
is true or not. 
(3) Speech analysis by the Founding Fathers. 

Reference:
Micklethwait, J., & Wooldridge, A. (2015). The fourth revolution: the global race to reinvent the state. New York, NY: Penguin Books. 

Code Reference:
Wk2-Tutorial-TextMining. (n.d.). Retrieved January 30, 2018, from https://github.com/TZstatsADS/ADS_Teaching/blob/master/Tutorials/wk2-TextMining/doc/wk2-Tutorial-TextMining.Rmd



0.) Package Preparation
Download all the packages needed for data analyses purposes. 
```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}


# load packages
library("rvest")
library("tibble")
# You may need to run
# sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
# in order to load qdap
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("xml2")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

1.) Find the inaugural speech date and make sure that every speeches are downloaded
```{r}
### Inaugural speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches. 
inaug=f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
```

2.) Using speech metadata
```{r}
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
```

3.) Scrap the text
```{r}
speech.list=inaug.list
speech.list$type=c(rep("inaug", nrow(inaug.list)))
speech.url=inaug
speech.list=cbind(speech.list, speech.url)
```

```{r}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
  text <- read_html(speech.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  speech.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/fulltext/", 
                     speech.list$type[i],
                     speech.list$File[i], "-", 
                     speech.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}
```

4.) Generate list of sentences
```{r, message=FALSE, warning=FALSE}
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    # colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```

Some non-sentences exist in raw data due to erroneous extra end-of-sentence marks. 
```{r}
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count)) 

```

5.) Data Analysis 
Average length of speech Data Visualization each president
```{r}
sentence.list$FileOrdered=reorder(sentence.list$File,sentence.list$word.count, mean,order=T)

beeswarm(word.count~FileOrdered, 
         data=sentence.list,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.8, cex.lab=0.8,
         spacing=5/nlevels(sentence.list$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Presidential speeches")

```

6.) Topic Modelling

Divide topic based on political parties and Founding Fathers

```{r}
rep.sentence.list = sentence.list[sentence.list$Party == 'Republican',]
dem.sentence.list = sentence.list[sentence.list$Party == 'Democratic',]

## Founding Fathers
founding.fathers <- c('GeorgeWashington', 'JohnAdams', 'ThomasJefferson', 'JamesMadison')
fathers.i <- sentence.list$FileOrdered %in% founding.fathers
fathers.list <- sentence.list[fathers.i,]
fathers.list$Party <- 'FoundingFathers'

political.list <- list(rep.sentence.list, dem.sentence.list, fathers.list)
```

LDA
```{r}
for (i in 1:length(political.list)){
        this.list <- political.list[[i]]
        this.party = unique(this.list$Party)
        this.party <- this.party[!is.na(this.party)]
        # Party LDA
        corpus.list=this.list[2:(nrow(this.list)-1), ]
        sentence.pre=this.list$sentences[1:(nrow(this.list)-2)]
        sentence.post=this.list$sentences[3:(nrow(this.list)-1)]
        corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
        rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
        rm.rows=c(rm.rows, rm.rows-1)
        #corpus.list=corpus.list[-rm.rows, ]
        
        # Text Mining
        docs <- Corpus(VectorSource(corpus.list$snipets))
        
        
        
        #remove potentially problematic symbols
        docs <-tm_map(docs,content_transformer(tolower))
       
        
        #remove punctuation
        docs <- tm_map(docs, removePunctuation)
        
        
        #Strip digits
        docs <- tm_map(docs, removeNumbers)
        
        
        #remove stopwords
        docs <- tm_map(docs, removeWords, stopwords("english"))
        
        
        #remove whitespace
        docs <- tm_map(docs, stripWhitespace)
        
        
        #Stem document
        docs <- tm_map(docs,stemDocument)
        
        
        ### Topic modelling
        dtm <- DocumentTermMatrix(docs)
        #convert rownames to filenames#convert rownames to filenames
        rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                               corpus.list$Term, corpus.list$sent.id, sep="_")
        
        rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
        
        dtm  <- dtm[rowTotals> 0, ]
        corpus.list=corpus.list[rowTotals>0, ]
        
        
        # Run LDA
        #Set parameters for Gibbs sampling
        burnin <- 20
        iter <- 2000
        thin <- 50
        seed <- 013018
        nstart <- 1
        best <- TRUE
        
        #Number of topics
        k <- 9
        
        #Run LDA using Gibbs sampling
        ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                          seed = seed, best=best,
                                                          burnin = burnin, iter = iter, 
                                                          thin=thin))
        #write out results
        #docs to topics
        ldaOut.topics <- as.matrix(topics(ldaOut))
        table(c(1:k, ldaOut.topics))
        write.csv(ldaOut.topics,file=paste0("LDAGibbs",k,this.party,"DocsToTopics.csv"))
      
        #top 20 terms in each topic
        ldaOut.terms <- as.matrix(terms(ldaOut,20))
        write.csv(ldaOut.terms,file=paste0("LDAGibbs",k,this.party,"TopicsToTerms.csv"))
        print(paste("Words Distributions for ", this.party))
        print(ldaOut.terms)
        #probabilities associated with each topic assignment
        topicProbabilities <- as.data.frame(ldaOut@gamma)
        write.csv(topicProbabilities,file=paste0("LDAGibbs",k,this.party,"TopicProbabilities.csv"))
}

```

7.) Analysis of the most words used using WordCloud
WordCloud for Democratic Party:
```{r}
#Read the file
library(tidytext)
library(wordcloud)
preprocess_data<- function(sentence.list){
  
  docs <- Corpus(VectorSource(corpus.list$snipets))
  #remove potentially problematic symbols
  docs <-tm_map(docs,content_transformer(tolower))
  
  #remove punctuation
  docs <- tm_map(docs, removePunctuation)
  
  #Strip digits
  docs <- tm_map(docs, removeNumbers)
  
  #remove stopwords
  docs <- tm_map(docs, removeWords, stopwords("english"))
  
  #remove whitespace
  docs <- tm_map(docs, stripWhitespace)
  
  #Stem document
  docs <- tm_map(docs,stemDocument)
}
#Creating the WordCloud
docs <- preprocess_data(dem.sentence.list)
tdm.all<-TermDocumentMatrix(docs)
tdm.tidy=tidy(tdm.all)
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))

wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(4,0.5),
          max.words=50,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues")
         )

```

WordCloud for Republican Party:
```{r}
#Read the file
library(tidytext)
library(wordcloud)
preprocess_data<- function(sentence.list){
  
  docs <- Corpus(VectorSource(corpus.list$snipets))
  #remove potentially problematic symbols
  docs <-tm_map(docs,content_transformer(tolower))
  
  #remove punctuation
  docs <- tm_map(docs, removePunctuation)
  
  #Strip digits
  docs <- tm_map(docs, removeNumbers)
  
  #remove stopwords
  docs <- tm_map(docs, removeWords, stopwords("english"))
  
  #remove whitespace
  docs <- tm_map(docs, stripWhitespace)
  
  #Stem document
  docs <- tm_map(docs,stemDocument)
}
#Creating the WordCloud
docs <- preprocess_data(rep.sentence.list)
tdm.all<-TermDocumentMatrix(docs)
tdm.tidy=tidy(tdm.all)
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))

wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(4,0.5),
          max.words=50,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues")
         )

```

Analysis:
1.) From the graph from part (5), it is clear that the number of words in the sentences
in the sentences in presidential speeches are decreasing over time.

2.) The analysis of the Republican ideology based on topic modelling:

-Freedom
Republicans generally prefers freedom compared to anything else. This what differentiates 
Republicans and Democrats. The way they defines freedom is that the government should not 
interfere with peoples' life. From the first column, it is clear the central tenents of 
Republican ideology, faith and freedom.

-Faith
This is central to Republican Party's ideology that it promotes social conservatism. Social 
conservatism generally promotes preservation of traditional religious beliefs. In this case,
the words are similar to what religious people talk about, in particular Christianity. 

-Free Market
The Republican Party encourages free trade as its economic policies. From Topic 6, it is clear 
that the words that are related to that: trade, business, increase, import. 

-Security/Defense
From Topic 4, it is clear that security is the platform for the Republicans. There is a big 
faction of Republican Party called neoconservatism. Neoconservatism ideology promotes high 
security for the world provided by American military.The topic 4 represents the 
words that are related to country, world, and security issues. 

3.) The analysis of the Democratic ideology based on topic modelling:

-Progessivism
The Democratic Party represents the progress of America. This goes back to their main 
ideology of social liberalism. The party is currently supporting LGBT rights, affirmative 
action, path of citizenship for illegal/undocumented immigrants. From Tppic 1 and 5, it is 
clear that there are words like progressive, new, change. 

-Equality
This is related to progressivism that I have described above. From Topic 4, it is clear that
this is the case because equality is in the same bracket as the government. The Democratic 
Party believes that the government's job is to provide social equity for all citizens. 

4.) The analysis of Founding fathers' ideology based on topic modelling:

-Function of government
State, consitute, Happy, love in Topic 2 provides what the Founding Fathers' belief 
about function of government. In the Declaration of Independence, the founding fathers'
wrote about the function of government is to provide life, liberty, and the pursuit of 
happiness. 

-Independence
At that time, the United States is still a new country and they just had war with Britain.
So, in topic 3, they are talking about building a new country after war. Words that 
are associated with independence are nation, war, success. 

-Rationalism/Deism
This was what the founding fathers' belief with general population in that time. Founding 
Fathers that became presidents mostly believed that rationalism trumps everything. This 
evidence is corroborated by the fact that they are all deists. Deists believes that anything
should be explained in rational and empirical sense. This is clear from Topic 9. 

5.) WordCloud:
There is no sufficient evidence that we can find about the difference about most used 
words between two major parties in America right now. 








